{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9134a9e",
   "metadata": {},
   "source": [
    "# Tutorial Content\n",
    "\n",
    "\n",
    "This notebook will walk you through steps to read in and organize velocity data in a raster format using xarray and rioxarray tools\n",
    "\n",
    "First, lets install the python libraries that were listed on the [Software](software.ipynb) page:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af56a8e",
   "metadata": {},
   "source": [
    "# new notebook structure (still working on)**\n",
    "\n",
    "- brief description of notebook at top (section 1)\n",
    "- raster datastream 1, ITS_LIVE geotiff ingesting and organizing (section 2)\n",
    "    - look at dims, vars, crs etc\n",
    "- raster datastream 2, ITS_IVE netcdf ingesting and organizing (section 3)\n",
    "    - look at dims, vars, crs etc\n",
    "- read in vector data (section 4)\n",
    "- working with projections (section 5)\n",
    "    - raster datasets have different projections (UTM and Asia_North_Lambert_Conformal_Conic)\n",
    "    - should i put them into same CRS or have a different projected geodataframe of glaciers for each velocity ds, and totally separate workflows...?\n",
    "        - if same crs, which prefereable btw UTM and asia norht lambert conformal conic\n",
    "- once both raster types are in workable formats, start on reading in vectors etc (section 6...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41dc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "from geocube.api.core import make_geocube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '/Users/emmamarshall/Desktop/phd_research/siparcs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295748e",
   "metadata": {},
   "source": [
    "## Raster datastream 1: ITS_LIVE (Geotiff)\n",
    "\n",
    "This section contains a workflow for reading in and organizing ITS_LIVE glacier velocity data that is accessed in geotiff format from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a409ba4",
   "metadata": {},
   "source": [
    "The velocity data we are using is broken into individual velocity components. That means that for the same spatial footprint, we have a file containing ice movement in the x direction and a file containing ice movement in the y direction. We need information from both of these files so we write a function to bring both files into the jupyter notebook, then organize them so that we can see the movement of ice in both the x and y directions as well as the magnitude of the ice velocity (speed). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_to_speed(vx_path, vy_path):\n",
    "    '''this function reads in x,y components of velocity, generates speed variable. return xarray\n",
    "    dataset w/ x,y, speed variables. function will break if vx,vy objects don\"t have same x,y coords'''\n",
    "    \n",
    "    vy_da = rxr.open_rasterio(vy_path, masked=False).squeeze()\n",
    "    vx_da = rxr.open_rasterio(vx_path, masked=False).squeeze()\n",
    "    \n",
    "    ds_gen = xr.Dataset()\n",
    "    ds_gen['vx'] = vx_da\n",
    "    ds_gen['vy'] = vy_da\n",
    "    sp = np.sqrt((ds_gen['vx'].data**2) + ds_gen['vy'].data**2)\n",
    "    ds_gen['sp'] = (['x','y'], sp.T)\n",
    "    \n",
    "    return ds_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91a05a",
   "metadata": {},
   "source": [
    "Let's break down what exactly the above function is doing:\n",
    "\n",
    "First, we see that it takes two inputs: vx_path and vy_path. These paths point to where on our computer the different files are stored. \n",
    "\n",
    "In the first two lines of the function we use rioxarray to read in the x- and the y-component files as **xarray.DataArrays**\n",
    "\n",
    "After that, we initialize a new object, *ds_gen*, which is a **xarray.DataSet**. We then add a variable to ds_gen called 'vx' and assign the vx_da object to that variable. We do the same for vx_da. Now, we have made a dataset that is composed of the two data arrays that we read in from file. \n",
    "\n",
    "We are also interested in speed, so we take the equation for computing magnitude of velocity and add a third variable (DataArray) to our Dataset. \n",
    "\n",
    "This will add a variable defined by the equation:\n",
    "\n",
    "            vv = (vx^2 + vy^2)^1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b352c9",
   "metadata": {},
   "source": [
    "Let's execute the function and take a look at the object it returns\n",
    "\n",
    "First, define the inputs to your function. These are the paths to the x and the y data on your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "n45_vy_path = gen_path + '/mynewbook/gardner_data/N45_0240m_vy.tiff'\n",
    "n45_vx_path = gen_path + 'mynewbook/gardner_data/N45_0240m_vx.tiff'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63539ae8",
   "metadata": {},
   "source": [
    "And run the function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb698670",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_45n = components_to_speed(n45_vx_path, n45_vy_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_45n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4400bd",
   "metadata": {},
   "source": [
    "What is the CRS of this object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52180b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_45n.spatial_ref.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf218a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_45n.dims)\n",
    "print('---')\n",
    "print(ds_45n.coords)\n",
    "print('---')\n",
    "print(ds_45n.variables)\n",
    "print('---')\n",
    "print(ds_45n.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e67ab3",
   "metadata": {},
   "source": [
    "## Raster datastream 2: ITS_LIVE (netcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive = rxr.open_rasterio('/Users/emmamarshall/Desktop/phd_research/siparcs/HMA_G0120_0000.nc').squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e56caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive.rio.crs.from_wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75335769",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100f674",
   "metadata": {},
   "source": [
    "## Vector data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb739146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in vector data \n",
    "se_asia = gpd.read_file('/Users/emmamarshall/Downloads/15rgi60SouthAsiaEast/15_rgi60_SouthAsiaEast.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105f141",
   "metadata": {},
   "source": [
    "How many glaciers are in this dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(se_asia['RGIId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaa855",
   "metadata": {},
   "source": [
    "What coordinate reference system is this dataframe in? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183f2ce",
   "metadata": {},
   "source": [
    "## Handling projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d94b63",
   "metadata": {},
   "source": [
    "Let's project this dataframe to match the CRS of the itslive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia_prj = se_asia.to_crs('+proj=lcc +lat_1=15 +lat_2=65 +lat_0=30 +lon_0=95 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m no_defs'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rasterize_vector(gpdf, utm_code, raster_obj): \n",
    "    \n",
    "    #read in gpdf from shp file\n",
    "    #gpdf = gpd.read_file(vector_path)\n",
    "    #project to local utm\n",
    "    gpdf_utm = gpdf.to_crs(f'EPSG:{utm_code}')\n",
    "    #use index as a unique key for each glacier\n",
    "    gpdf_utm['Integer_ID'] = gpdf_utm.index.astype(int)\n",
    "    #print(gpdf_utm['Integer_ID'])\n",
    "    \n",
    "    #rasterize glacier vector by unique id \n",
    "\n",
    "    out_grid = make_geocube(\n",
    "            vector_data = gpdf_utm,\n",
    "            measurements = ['Integer_ID'],\n",
    "            like = raster_obj['sp'] #need to specify a var here, not sure best way to do that\n",
    "            )\n",
    "    \n",
    "    #now merge the rasterized vector and the original raster togehter into a geocube\n",
    "    out_grid['speed'] = (raster_obj.dims, raster_obj.sp.values, raster_obj.attrs, raster_obj.encoding)\n",
    "    \n",
    "    #now, get velocity statistics of each 'region' (integer) using the mask\n",
    "    #grouped_ID = out_grid.drop('spatial_ref').groupby(out_grid.Integer_ID)\n",
    "\n",
    "    #compute zonal stats groupedd by ID\n",
    "    #grid_mean_sp = grouped_ID.mean().rename({'speed': 'speed_mean'})\n",
    "    #grid_min_sp = grouped_ID.min().rename({'speed': 'speed_min'})\n",
    "    #grid_max_sp = grouped_ID.max().rename({'speed': 'speed_max'})\n",
    "    #grid_std_sp = grouped_ID.max().rename({'speed': 'speed_std'})\n",
    "    \n",
    "    #merge each zonal stat xr obj into a single xr obj, convert to pandas df\n",
    "    #zonal_stats = xr.merge([grid_mean_sp, grid_min_sp, grid_max_sp, grid_std_sp]).to_dataframe()\n",
    "    #zonal_stats = zonal_stats.reset_index()\n",
    "    \n",
    "   # return zonal_stats\n",
    "    return out_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f03fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterize_vector_seasia = rasterize_vector(se_asia, 32645, ds_45n)\n",
    "rasterize_vector_seasia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rasterize_vector_seasia.Integer_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project to utm\n",
    "se_asia_utm = se_asia.to_crs('EPSG:32645')\n",
    "#make a col in df that is a unique integer ID (from index) for each glacier\n",
    "se_asia_utm['Integer_ID'] = se_asia_utm.index.astype(int)\n",
    "#double checking that all glaciers are assigned an ID\n",
    "se_asia_utm.plot.scatter(x='Integer_ID', y='Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f0fec",
   "metadata": {},
   "source": [
    "The plot above shows the mean ice speed of every glacier in the geodataframe object, **se_asia**, that lies within the spatial extent the velocity object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ed29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rasterize glacier vector by unique id \n",
    "#\n",
    "out_grid_se_asia = make_geocube(\n",
    "            vector_data = se_asia_utm,\n",
    "            measurements = ['Integer_ID'],\n",
    "            like = ds_45n['sp']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now merge the rasterized vector and the original raster togehter into a geocube\n",
    "out_grid_se_asia['speed'] = (ds_45n.dims, ds_45n.sp.values, ds_45n.attrs, ds_45n.encoding)\n",
    "out_grid_se_asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to figure out why 1300 glaciers or so get dropped\n",
    "print(len(out_grid_se_asia.Integer_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, get velocity statistics of each 'region' (integer) using the mask\n",
    "grouped_ID = out_grid_se_asia.drop('spatial_ref').groupby(out_grid_se_asia.Integer_ID)\n",
    "grouped_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mean_sp = grouped_ID.mean().rename({'speed': 'speed_mean'})\n",
    "grid_median_sp = grouped_ID.median().rename({'speed': 'speed_median'})\n",
    "grid_min_sp = grouped_ID.min().rename({'speed': 'speed_min'})\n",
    "grid_max_sp = grouped_ID.max().rename({'speed': 'speed_max'})\n",
    "grid_std_sp = grouped_ID.max().rename({'speed': 'speed_std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_stats = xr.merge([grid_mean_sp, grid_median_sp, grid_min_sp, grid_max_sp, grid_std_sp]).to_dataframe()\n",
    "zonal_stats = zonal_stats.reset_index()\n",
    "zonal_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, trying to merge zonal stats df back with original glacier df on integer_ID col\n",
    "se_asia_glacier_data = se_asia_utm.merge(zonal_stats, on='Integer_ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_stats['speed_mean']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "se_asia_glacier_data.plot.scatter(x='Integer_ID',y = 'speed_mean', c = 'darkblue', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabab934",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_stats['speed_mean'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia_glacier_data.plot(column='speed_mean', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0187d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96158a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-mynewbook-py",
   "language": "python",
   "name": "conda-env-mynewbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
