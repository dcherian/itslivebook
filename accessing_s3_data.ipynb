{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing ITS_LIVE data via S3 bucket\n",
    "\n",
    "This notebook will demonstrate how to access cloud-hosted Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) data from AWS S3 buckets. Here you will find examples of how to successfully access cloud-hosted data as well as some common errors and issues you may run into along the way, what they mean, and how to resolve them. \n",
    "\n",
    "Learning goals:\n",
    "- accessing and inspecting data stored in s3 buckets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test etst test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6/11 still to add\n",
    " - probably need to clean up the individual data cube builds at the beginning,  introduce read_in_s3() sooner and use that instead of by hand, add try: , except: to read_in_s3()\n",
    " - get_bbox() should show something about the granule id/location as a text printout ideally as well\n",
    " - maybe show a basic beginning example of looking at a single scene, plotting a few different time steps to show variability in coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding / selecting data\n",
    "\n",
    "The ITS_LIVE project details a number of data access options on their [website](https://its-live.jpl.nasa.gov/#access). Here, we will be accessing ITS_LIVE data in the form of `zarr` data cubes that are stored in **s3 buckets** hosted by Amazon Web Services (AWS). \n",
    "\n",
    "Let's begin by looking at the [GeoJSON Data Cubes Catalog](https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json). \n",
    "This catalog contains spatial information and properties of ITS_LIVE data cubes as well as the url used to access each cube. Let's take a look at the entry for a single data cube and the information that it contains:\n",
    "\n",
    "![itslive_info](images/screengrab_itslive_catalog_entry.png)\n",
    "\n",
    "The top portion of the picture shows the spatial extent of the data cube in lat/lon units. Below that, we have properties such as the epsg code of the coordinate reference system, the spatial footprint in projected units and the url of the zarr object. \n",
    "\n",
    "Let's take a look at the url more in-depth: \n",
    "\n",
    "![itslive_url](images/itslive_url.png)\n",
    "\n",
    "From this link we can see that we are looking at its_live data located in an s3 bucket hosted by amazon AWS. We cans see that we're looking in the data cube directory and what seems to be version 2. The next bit gives us information about the global location of the cube (N40E080). The actual file name `ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr` tells us that we are looking at ice velocity data (its_live also has elevation data), in the CRS associated with EPSG 32645 (this code indicates UTM zone 45N). X250000_Y4750000 tells us more about the spatial footprint of the datacube within the UTM zone. \n",
    "\n",
    "**NOTE**\n",
    "This catalog provides http links to the zarr objects. To successfully point to the objects that we're looking for in s3 buckets, we need to make a few changes to the links:\n",
    "- replace 'http' with 's3'\n",
    "- delete '.s3.amazonaws.com' </br>\n",
    "\n",
    "so the correct url should read: </br>\n",
    "\n",
    "    `s3://its-live-data/datacubes/v02/N40E080/ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing s3 data from python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found the url associated with the tile we want to access, let's try to open the data cube using `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception",
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "url_x25_y475 = 's3://its-live-data/datacubes/v02/N40E080/ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr'\n",
    "data_cube_x25_y475 = xr.open_dataset(url_x25_y475, engine = 'zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this doesn't quite work. We need to specify a bit more information for xarray to be able to access and load the data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cube_x25_y475 = xr.open_dataset(url_x25_y475, engine= 'zarr',\n",
    "                                    storage_options = {'anon':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one worked! Let's stop here and define a function that we can use for a quick inspection of this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_single(input_xr):\n",
    "    \n",
    "    '''Takes input xr object (from itslive data cube), plots a quick map of the footprint. \n",
    "    currently only working for granules in crs epsg 32645'''\n",
    "\n",
    "    xmin = input_xr.coords['x'].data.min()\n",
    "    xmax = input_xr.coords['x'].data.max()\n",
    "\n",
    "    ymin = input_xr.coords['y'].data.min()\n",
    "    ymax = input_xr.coords['y'].data.max()\n",
    "\n",
    "    pts_ls = [(xmin, ymin), (xmax, ymin),(xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "    #print(input_xr.mapping.spatial_epsg)\n",
    "    #print(f\"epsg:{input_xr.mapping.spatial_epsg}\")\n",
    "    crs = f\"epsg:{input_xr.mapping.spatial_epsg}\"\n",
    "    #crs = {'init':f'epsg:{input_xr.mapping.spatial_epsg}'}\n",
    "    #crs = 'epsg:32645'\n",
    "    #print(crs)\n",
    "\n",
    "    polygon_geom = Polygon(pts_ls)\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom]) \n",
    "    polygon = polygon.to_crs('epsg:4326')\n",
    "\n",
    "    bounds = polygon.total_bounds\n",
    "    bounds_format = [bounds[0]-15, bounds[2]+15, bounds[1]-15, bounds[3]+15]\n",
    "\n",
    "    states_provinces = cfeature.NaturalEarthFeature(\n",
    "        category = 'cultural',\n",
    "        name = 'admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none'\n",
    "    )\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection = ccrs.PlateCarree())\n",
    "    ax.stock_img()\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(states_provinces)\n",
    "\n",
    "    ax.set_extent(bounds_format, crs = ccrs.PlateCarree())\n",
    "\n",
    "    polygon.plot(ax=ax, facecolor = 'none', edgecolor='red', lw=1.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a quick function for reading in s3 objects from http urls. This will come in handy when we're trying to test multiple urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_s3(http_url):\n",
    "    s3_url = http_url.replace('http','s3')\n",
    "    s3_url = s3_url.replace('.s3.amazonaws.com','')\n",
    "\n",
    "    datacube = xr.open_dataset(s3_url, engine = 'zarr',\n",
    "                                storage_options={'anon':True},\n",
    "                                chunks = 'auto')\n",
    "\n",
    "    return datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the cube we've already read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bbox_single(data_cube_x25_y475)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see where this granule lies. Let's try another url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "\"output_scroll\"",
     "\"raises_exception\""
    ]
   },
   "outputs": [],
   "source": [
    "#this url doen'st work\n",
    "url_x35_y305 = 'http://its-live-data.s3.amazonaws.com/datacubes/v02/N20E080/ITS_LIVE_vel_EPSG32645_G0120_X350000_Y3350000.zarr'\n",
    "dc_x35_y305 = read_in_s3(url_x35_y305)\n",
    "                                    #engine= 'zarr',\n",
    "                                    #storage_options = {'anon':True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This link doesn't work. From the error we're getting, it looks like it's not working because the zarr file isn't in the location that we're pointing to with the url we supply. As cloud-based datasets are developed and maintained, the locations of specific objects will change and sometimes metadata doesn't always catch up, so this may be an error you'll encounter more than once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another url: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "'raises_exception'",
     "\"raises_exception\""
    ]
   },
   "outputs": [],
   "source": [
    "url_x35_y315 = 'http://its-live-data.s3.amazonaws.com/datacubes/v02/N20E080/ITS_LIVE_vel_EPSG32645_G0120_X350000_Y3150000.zarr'\n",
    "\n",
    "dc_x35_y315 = read_in_s3(url_x35_y315)\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching catalog of s3 urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that some zarr objects are located at the supplied urls but that some aren't. We have the entire catalog, but it would be a pain to look through each of those urls individually so let's write a function to find out which urls are working and which arent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url:\n",
    "    itslive_catalog = json.loads(url.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the json object has the form of nested dictionaries that contain information about all of the its_live datacubes. Here we'll show two options for filtering the catalog: one for selecting granules that contain a specific point and a second that returns all granules within a single UTM zone (specified by epsg code). This will let us take stock of the spatial coverage of data cubes located at **working urls** within a certain UTM zone. You could easily tewak this function (or write your own!) to select granules based on different properties. Play around with the `itslive_catalog` object to become more familiar with the data it contains and different options for indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data cubes by UTM zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_url = itslive_catalog['features'][0]['properties']['zarr_url']\n",
    "s3_url_test = http_url.replace('http','s3').replace('.s3.amazonaws.com','')\n",
    "s3_url_test\n",
    "\n",
    "pathExists = os.path.lexists(s3_url_test)\n",
    "pathExists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs  = s3fs.S3FileSystem(anon=True)\n",
    "fs\n",
    "\n",
    "fs.lexists(s3_url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(s3_url_test, engine='zarr',\n",
    "                storage_options = {'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_granules_by_zone(input_dict, epsg_code):\n",
    "    '''This function takes a dictionary (itslive catalog geojson) and a epsg code referencing region of interest. \n",
    "    returns list of urls corresponding to datacubes stored in s3 buckets where links *exist*'''\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "    url_ls = []\n",
    "    for granule in range(len(input_dict['features'])):\n",
    "        if input_dict['features'][granule]['properties']['data_epsg'] == epsg_code:\n",
    "\n",
    "            #format question - better to condense this into 1 line or break into 3 to be more readable?\n",
    "            http_url = input_dict['features'][granule]['properties']['zarr_url']\n",
    "            s3_url = http_url.replace('http','s3').replace('.s3.amazonaws.com','')\n",
    "\n",
    "            if fs.lexists(s3_url) == True:\n",
    "                url_ls.append(s3_url)\n",
    "\n",
    "    return url_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_granules_by_epsg(input_dict, input_epsg):\n",
    "    '''this function takes a dictionary that is a representation of itslive data cube geojson catalog as well as an \n",
    "    epsg code. it makes an intermediate object that is a list of all s3 urls for that utm region/epsg code. it\n",
    "    then tried to open each zarr url as an xarray dataset and returns a list of the ones that are able to be opened\n",
    "    successuflly (ie. the zarr file exists at the s3 url). each element of the return list is an xr dataset'''\n",
    "    \n",
    "    epsg_ls = []\n",
    "        \n",
    "    for granule in range(len(input_dict['features'])):\n",
    "        \n",
    "        if input_dict['features'][granule]['properties']['data_epsg'] == input_epsg:\n",
    "            \n",
    "            http_url = input_dict['features'][granule]['properties']['zarr_url']\n",
    "            s3_url = http_url.replace('http', 's3')\n",
    "            s3_url = s3_url.replace('.s3.amazonaws.com','')\n",
    "            epsg_ls.append(s3_url)\n",
    "     \n",
    "    datacube_ls = []\n",
    "    for element in range(len(epsg_ls)):\n",
    "\n",
    "        try:\n",
    "            #datacube_ls.append(read_in_s3(epsg_ls[element]))\n",
    "            datacube_ls.append(xr.open_dataset(epsg_ls[element], engine = 'zarr',\n",
    "                                storage_options={'anon':True}))\n",
    "        except ValueError:\n",
    "            print('nothing there ¯\\_(ツ)_/¯ ')\n",
    "            \n",
    "    return datacube_ls\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "granule_ls_32645 = find_granules_by_epsg(itslive_catalog, 'EPSG:32645')\n",
    "granule_ls_32645[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list of xarray objects that correspond to all of the zarr urls for that region that were able to be successfully accessed.\n",
    "\n",
    "Let's write a function to visualize the coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_group(input_ls, bounds = [-180, 180, -90, 90]): \n",
    "    \n",
    "    '''plots the spatial extents of a list of datacubes'''\n",
    "    \n",
    "    poly_ls = []\n",
    "    \n",
    "    for xr_obj in range(len(input_ls)):\n",
    "        '''Takes input xr object (from itslive data cube), plots a quick map of the footprint. \n",
    "        currently only working for granules in crs epsg 32645'''\n",
    "\n",
    "        xmin = input_ls[xr_obj].coords['x'].data.min()\n",
    "        xmax = input_ls[xr_obj].coords['x'].data.max()\n",
    "        ymin = input_ls[xr_obj].coords['y'].data.min()\n",
    "        ymax = input_ls[xr_obj].coords['y'].data.max()\n",
    "\n",
    "        pts_ls = [(xmin, ymin), (xmax, ymin),(xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "        \n",
    "        crs = f\"epsg:{input_ls[xr_obj].mapping.spatial_epsg}\" #should be format: 'epsg:32645'\n",
    "        #print(crs)\n",
    "\n",
    "        polygon_geom = Polygon(pts_ls)\n",
    "        polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom]) \n",
    "        polygon = polygon.to_crs('epsg:4326')\n",
    "        poly_ls.append(polygon)\n",
    "\n",
    "    bounds_format = [bounds[0], bounds[1], bounds[2], bounds[3]]\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection = ccrs.PlateCarree())\n",
    "    ax.stock_img()\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    ax.set_extent(bounds_format, crs = ccrs.PlateCarree())\n",
    "    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.left_labels = False\n",
    "    gl.xlines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-180, -45, 0, 45, 180])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 15, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'color': 'black'}\n",
    "\n",
    "    for element in range(len(poly_ls)):\n",
    "            \n",
    "        #polygon.plot(ax=ax, facecolor = 'none', edgecolor='red', lw=1.)\n",
    "        poly_ls[element].plot(ax=ax, facecolor='none', edgecolor='red', lw=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bbox_group(granule_ls_32645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the region of interest, we can do that by specifying bounds when we call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bbox_group(granule_ls_32645, [70, 98, 20, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting granules by a single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#might need to add a try, except statement to this in case the url doesn't work, or maybe to read_in_s3 fn...\n",
    "def find_granule_by_point(input_dict, input_point): #[lon,lat]\n",
    "    '''Takes an inputu dictionary (a geojson catalog) and a point to represent AOI.\n",
    "    this returns a list of the s3 urls corresponding to zarr datacubes whose footprint covers the AOI'''\n",
    "    #print([input_points][0])\n",
    "    \n",
    "    target_granule_urls = []\n",
    "    #Point(coord[0], coord[1])\n",
    "    #print(input_point[0])\n",
    "    #print(input_point[1])\n",
    "    point_geom = Point(input_point[0], input_point[1])\n",
    "    #print(point_geom)\n",
    "    point_gdf = gpd.GeoDataFrame(crs='epsg:4326', geometry = [point_geom])\n",
    "    for granule in range(len(input_dict['features'])):\n",
    "        \n",
    "        #print('tick')\n",
    "        bbox_ls = input_dict['features'][granule]['geometry']['coordinates'][0]\n",
    "        bbox_geom = Polygon(bbox_ls)\n",
    "        bbox_gdf = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry = [bbox_geom])\n",
    "        \n",
    "        #if poly_gdf.contains(points1_ls[poly]).all() == True:\n",
    "\n",
    "        if bbox_gdf.contains(point_gdf).all() == True:\n",
    "            #print('yes')\n",
    "            target_granule_urls.append(input_dict['features'][granule]['properties']['zarr_url'])\n",
    "        else:\n",
    "            pass\n",
    "            #print('no')\n",
    "    return target_granule_urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_url_ls = find_granule_by_point(itslive_catalog, [86.7, 28.07])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this function returned a single url corresponding to the data cube covering the point we supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_url_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `read_in_s3` function we defined to open the datacube as an `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_dc = read_in_s3(nepal_url_ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then the `get_bbox_single` function to take a look at the footprint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bbox_single(nepal_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we know how to access the its_live data cubes for a given region as well as at a specific point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6/14 Trying to calc/show seasonal mean velocities but not working yet... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make subset of just a few time steps to start\n",
    "nepal_sub = nepal_dc.isel(mid_date = slice(0,12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore dt accessors a bit\n",
    "nepal_sub.mid_date.dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate seasonal means using groupby of subset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_sub\n",
    "\n",
    "#subset the vars we're interested in, remove string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, split the data into groups, here that is seasons\n",
    "season_gb = nepal_sub.groupby(nepal_sub.mid_date.dt.season)\n",
    "season_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to apply to each group\n",
    "def middate_mean(a):\n",
    "    return a.mean(dim='mid_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "\"raises_exception\""
    ]
   },
   "outputs": [],
   "source": [
    "#use map to apply function to groups\n",
    "\n",
    "season_gb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "mynewbook",
   "language": "python",
   "name": "mynewbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
