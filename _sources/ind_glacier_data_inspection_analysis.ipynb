{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual glacier analysis 1\n",
    "\n",
    "This notebook will walk you through steps to read in and organize velocity data and clip it to the extent of a single glacier. The tools we will use include **xarray**, **rioxarray** and **geopandas**. \n",
    "\n",
    "To clip its_live data to the extent of a single glacier we will use a vector dataset of glacier outlines, the [Randolph Glacier Inventory](https://nsidc.org/data/nsidc-0770). These aren't cloud-hosted currently so you will need to download the data to your local machine. \n",
    "\n",
    "**Learning goals**\n",
    "come back and finish these, feel like this notebook has alot, is pretty disorganized.. </br>\n",
    "using xarray to read zarr data from s3 bucket\n",
    "- **`rio.clip()`** to clip raster by vector\n",
    "- viewing CRS, reprojecting and writing CRS data for various objects\n",
    "- dataset.where()\n",
    "- dataset.sel() using multiple conditions\n",
    "- groupby\n",
    "\n",
    "\n",
    "\n",
    "First, lets install the python libraries that were listed on the [Software](software.ipynb) page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import json\n",
    "import urllib.request\n",
    "from skimage.morphology import skeletonize\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in ITS_LIVE data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some of the functions we defiend in the data access notebook to read in data here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itslivetools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read in the catalog again:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url_catalog:\n",
    "    itslive_catalog = json.loads(url_catalog.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a single catalog entry:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to find the url that corresponds to the zarr datacube for a specific point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_in_s3()` function will read in a xarray dataset from a url to a zarr datacube when we're ready:\n",
    "\n",
    "I started with `chunk_size='auto'` but ran into issues. more about choosing good chunk sizes [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = itslivetools.find_granule_by_point(itslive_catalog, [84.56, 28.54])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = itslivetools.read_in_s3(url[0])\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading this in as a dask array. Let's take a look at the chunk sizes:\n",
    "\n",
    "**NOTE**: chunksizes shows the largest chunk size. chunks shows the sizes of all chunks along all dims, better if you have irregular chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the dask chunksize to `auto` at the `xr.open_dataset()` step will use chunk sizes that most closely resemble the structure of the underlying data. To avoid imposing a chunk size that isn't a good fit for the data, avoid re-chunking until we have selected a subset of our area of interest from the larger dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CRS of xr object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the time dimension (`mid_date` here). To start with we'll just print the first 10 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(10):\n",
    "    \n",
    "    print(dc.mid_date[element].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, it doesn't look like the time dimension is in chronological order, let's fix that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted = dc.sortby(dc['mid_date'])\n",
    "dc_timesorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read in the zarr datacube as a `xr.Dataset` we set the chunk sizes to `auto`. When we try to sort along the `mid_date` dimension this seems to become a problem and we get the warning above. \n",
    "\n",
    "At first it makes sens to follow the instructions in the warning message to avoid creating large chunks, but this creates some issues. If you want, turn the cell below to `code` and run it, you can see that this re-chunks the time dimension which isn't something that we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    dc_timesorted_false = dc.sortby(dc['mid_date'])\n",
    "    dc_timesorted_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(10):\n",
    "    \n",
    "    print(dc_timesorted.mid_date[element].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in vector data \n",
    "\n",
    "We are going to read in RGI region **15 (SouthAsiaEast)**. RGI data is downloaded in lat/lon coordinates. We will project it to match the CRS of the ITS_LIVE dataset and then select an individual glacier to begin our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_file('/Users/emarshall/Desktop/siparcs/data/nsidc0770_15.rgi60.SouthAsiaEast/15_rgi60_SouthAsiaEast.shp')\n",
    "se_asia.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project rgi data to match itslive\n",
    "se_asia_prj = se_asia.to_crs('EPSG:32645') #we know the epsg from looking at the 'spatial epsg' attr of the mapping var of the dc object\n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop RGI to ITS_LIVE extent\n",
    "- use get_bbox_single() from access nb but no plotting (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, get vector bbox of itslive\n",
    "\n",
    "bbox_dc = itslivetools.get_bbox_single(dc)\n",
    "bbox_dc['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project from latlon to local utm \n",
    "bbox_dc = bbox_dc.to_crs('EPSG:32645')\n",
    "bbox_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset rgi to bounds \n",
    "se_asia_subset = gpd.clip(se_asia_prj, bbox_dc)\n",
    "se_asia_subset\n",
    "se_asia_subset.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_vec = se_asia_subset.loc[se_asia_subset['RGIId'] == 'RGI60-15.04714']\n",
    "sample_glacier_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip ITS_LIVE dataset to individual glacier extent\n",
    "\n",
    "First, we need to use rio.write_crs() to assign a CRS to the itslive object. If we don't do that first the `rio.clip()` command will produce an error\n",
    "*Note*: it looks like you can only run write_crs() once, because it switches mapping from being a `data_var` to a `coord` so if you run it again it will produce a key error looking for a var that doesnt' exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted = dc_timesorted.rio.write_crs(f\"epsg:{dc_timesorted.mapping.attrs['spatial_epsg']}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_glacier_raster = dc_timesorted.rio.clip(sample_glacier_vec.geometry, sample_glacier_vec.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the clipped object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the clipped raster alongside the vector outline. To start with and for the sake of easy visualizing we will take the mean of the magnitude of velocity variable along the `mid_date` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,9))\n",
    "sample_glacier_vec.plot(ax=ax, facecolor='none', edgecolor='red');\n",
    "sample_glacier_raster.v.mean(dim=['mid_date']).plot(ax=ax);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the x and y components of velocity, again averaging over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols =2, figsize=(17,7))\n",
    "\n",
    "sample_glacier_raster.vx.mean(dim='mid_date').plot(ax=axs[0]);\n",
    "sample_glacier_raster.vy.mean(dim='mid_date').plot(ax=axs[1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.v_error.mean(dim=['mid_date']).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring ITS_LIVE data\n",
    "\n",
    "ITS_LIVE data cubes come with many (53!) variables that carry information about the estimated surface velocities and the satellite images that were used to generate the surface velocity estimates. We won't examine all of this information here but let's look at a litte bit.\n",
    "\n",
    "To start with, let's look at the satellite imagery used to generate the velocity data.\n",
    "\n",
    "We see that we have two `data_vars` that indicate which sensor that each image in the image pair at a certain time step comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.satellite_img1.data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.satellite_img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `satellite_img1` and `satellite_img2` variables are 1-dimensional numpy arrays corresponding to the length of the `mid_date` dimension of the data cube. You can see that each element of the array is a string corresponding to a different satellite:\n",
    "    `1A` = Sentinel 1A, `1B` = Sentinel 1B, `2A` = Sentinel 2A\n",
    "    `2B` = Sentinel 2B, `8.` = Landsat8 and `9.` = Landsat9\n",
    "    \n",
    "Let's re-arrange these string arrays into a format that is easier to work with.\n",
    "\n",
    "First, we'll make a set of all the different string values in the satellite image variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining velocity data from each satellite in `ITS_LIVE` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only wanted to look at the velocity estimates from landat8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_data = sample_glacier_raster.where(sample_glacier_raster['satellite_img1'] == '8.', drop=True)\n",
    "l8_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset.where()` at first seems appropriate to use for kind of operation but there's actually an easier way. Because we are selecting along a single dimension (`mid_date`), we can use xarray's `.sel()` method instead. This is more efficient and integrates with `dask` arrays more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_condition = sample_glacier_raster.satellite_img1.isin('8.')\n",
    "l8_subset = sample_glacier_raster.sel(mid_date=l8_condition)\n",
    "l8_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are looking at roughly a third of the original time steps. Let's take a look at the average speeds of the Landsat8-derived velocities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Landsat9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_condition = sample_glacier_raster.satellite_img1.isin('9.')\n",
    "\n",
    "l9_subset = sample_glacier_raster.sel(mid_date=l9_condition)\n",
    "l9_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 45 time steps have data from Landsat9, this makes sense because Landsat9 was just launched recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Sentinel 1 data. Note here we are selecting for 2 values instead of 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_condition = sample_glacier_raster.satellite_img1.isin(['1A','1B'])\n",
    "s1_subset = sample_glacier_raster.sel(mid_date = s1_condition)\n",
    "s1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_condition = sample_glacier_raster.satellite_img1.isin(['2A','2B'])\n",
    "s2_subset = sample_glacier_raster.sel(mid_date=s2_condition)\n",
    "s2_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking coverage along a dimension\n",
    "It would be nice to be able to scan/visualize and observe coverage of a variable along a dimension\n",
    "\n",
    "Attempting that below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to make a mask that will tell us all the possible 'valid' pixels. ie pixels over ice v. rock. Some versions of ITS_LIVE have a variable that indicates surface type that would be handy for this but the datacubes don't. Start by taking the mean of velocity pixels along the time dimension, take all pixels in this array that aren't NaN as ice pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pixels = sample_glacier_raster.v.count(dim=['x','y']).compute()\n",
    "valid_pix_max = valid_pixels.max().data\n",
    "\n",
    "cov = valid_pixels/valid_pix_max\n",
    "\n",
    "sample_glacier_raster['cov'] = cov\n",
    "sample_glacier_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many time steps are duplicates?, there are 16872 unique vals in mid_dates\n",
    "np.unique(sample_glacier_raster['mid_date'].data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,3))\n",
    "sample_glacier_raster.cov.plot(ax=ax, linestyle='None',marker = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.groupby('satellite_img1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.cov.groupby(sample_glacier_raster.satellite_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.groupby('mid_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flox.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = flox.xarray.xarray_reduce(\n",
    "    sample_glacier_raster.cov,\n",
    "    sample_glacier_raster.satellite_img1.compute(),\n",
    "    sample_glacier_raster.mid_date,\n",
    "    func=\"mean\",\n",
    "    fill_value=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(coverage.mid_date, coverage.satellite_img1, coverage, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal mean velocities for each year with groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_gb = sample_glacier_raster.groupby(sample_glacier_raster.mid_date.dt.year)\n",
    "year_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = flox.xarray.xarray_reduce(\n",
    "    sample_glacier_raster.cov,\n",
    "    sample_glacier_raster.satellite_img1.compute(),\n",
    "    sample_glacier_raster.mid_date,\n",
    "    func=\"mean\",\n",
    "    fill_value=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_season_mean_v = flox.xarray.xarray_reduce(\n",
    "    sample_glacier_raster.v,\n",
    "    sample_glacier_raster.mid_date.dt.year,\n",
    "    sample_glacier_raster.mid_date.dt.season,\n",
    "    func = 'mean',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_season_mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_season_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_yr = yr_season_mean.plot(\n",
    "        col='season',\n",
    "        row = 'year',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal mean velocities with groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first define the function we'll apply to each group\n",
    "def middate_mean(a):\n",
    "    return a.mean(dim='mid_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_gb = sample_glacier_raster.groupby(sample_glacier_raster.mid_date.dt.season).map(middate_mean)\n",
    "#add attrs to gb object\n",
    "seasons_gb.attrs = sample_glacier_raster.attrs #why didn't that work?\n",
    "seasons_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = seasons_gb.v.plot(\n",
    "    col='season',\n",
    "    vmax = 150\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-itslivetools-py",
   "language": "python",
   "name": "conda-env-itslivetools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
