{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual glacier analysis 1\n",
    "\n",
    "This notebook will walk you through steps to read in and organize velocity data and clip it to the extent of a single glacier. The tools we will use include **xarray**, **rioxarray** and **geopandas**. \n",
    "\n",
    "To clip its_live data to the extent of a single glacier we will use a vector dataset of glacier outlines, the [Randolph Glacier Inventory](https://nsidc.org/data/nsidc-0770). These aren't cloud-hosted currently so you will need to download the data to your local machine. \n",
    "\n",
    "**Learning goals**\n",
    "come back and finish these, feel like this notebook has alot, is pretty disorganized.. </br>\n",
    "using xarray to read zarr data from s3 bucket\n",
    "- **`rio.clip()`** to clip raster by vector\n",
    "- viewing CRS, reprojecting and writing CRS data for various objects\n",
    "- dataset.where()\n",
    "- dataset.sel() using multiple conditions\n",
    "- groupby\n",
    "\n",
    "\n",
    "\n",
    "First, lets install the python libraries that were listed on the [Software](software.ipynb) page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import json\n",
    "import urllib.request\n",
    "from skimage.morphology import skeletonize\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in ITS_LIVE data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some of the functions we defiend in the data access notebook to read in data here. First, let's read in the catalog again:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itslivetools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url_catalog:\n",
    "    itslive_catalog = json.loads(url_catalog.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a single catalog entry:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to find the url that corresponds to the zarr datacube for a specific point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def find_granule_by_point(input_dict, input_point): #[lon,lat]\n",
    "    '''Takes an inputu dictionary (a geojson catalog) and a point to represent AOI.\n",
    "    this returns a list of the s3 urls corresponding to zarr datacubes whose footprint covers the AOI'''\n",
    "    #print([input_points][0])\n",
    "    \n",
    "    target_granule_urls = []\n",
    "    #Point(coord[0], coord[1])\n",
    "    #print(input_point[0])\n",
    "    #print(input_point[1])\n",
    "    point_geom = Point(input_point[0], input_point[1])\n",
    "    #print(point_geom)\n",
    "    point_gdf = gpd.GeoDataFrame(crs='epsg:4326', geometry = [point_geom])\n",
    "    for granule in range(len(input_dict['features'])):\n",
    "        \n",
    "        #print('tick')\n",
    "        bbox_ls = input_dict['features'][granule]['geometry']['coordinates'][0]\n",
    "        bbox_geom = Polygon(bbox_ls)\n",
    "        bbox_gdf = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry = [bbox_geom])\n",
    "        \n",
    "        #if poly_gdf.contains(points1_ls[poly]).all() == True:\n",
    "\n",
    "        if bbox_gdf.contains(point_gdf).all() == True:\n",
    "            #print('yes')\n",
    "            target_granule_urls.append(input_dict['features'][granule]['properties']['zarr_url'])\n",
    "        else:\n",
    "            pass\n",
    "            #print('no')\n",
    "    return target_granule_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will read in a xarray dataset from a url to a zarr datacube when we're ready:\n",
    "\n",
    "I started with `chunk_size='auto'` but ran into issues. more about choosing good chunk sizes [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def read_in_s3(http_url):\n",
    "    s3_url = http_url.replace('http','s3')\n",
    "    s3_url = s3_url.replace('.s3.amazonaws.com','')\n",
    "\n",
    "    datacube = xr.open_dataset(s3_url, engine = 'zarr',\n",
    "                                storage_options={'anon':True},\n",
    "                                chunks = 'auto')\n",
    "\n",
    "    return datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_bbox_single(input_xr):\n",
    "    \n",
    "    '''Takes input xr object (from itslive data cube), plots a quick map of the footprint. \n",
    "    currently only working for granules in crs epsg 32645'''\n",
    "\n",
    "    xmin = input_xr.coords['x'].data.min()\n",
    "    xmax = input_xr.coords['x'].data.max()\n",
    "\n",
    "    ymin = input_xr.coords['y'].data.min()\n",
    "    ymax = input_xr.coords['y'].data.max()\n",
    "\n",
    "    pts_ls = [(xmin, ymin), (xmax, ymin),(xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "    #print(input_xr.mapping.spatial_epsg)\n",
    "    #print(f\"epsg:{input_xr.mapping.spatial_epsg}\")\n",
    "    crs = f\"epsg:{input_xr.mapping.spatial_epsg}\"\n",
    "    #crs = {'init':f'epsg:{input_xr.mapping.spatial_epsg}'}\n",
    "    #crs = 'epsg:32645'\n",
    "    #print(crs)\n",
    "\n",
    "    polygon_geom = Polygon(pts_ls)\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom]) \n",
    "    #polygon = polygon.to_crs('epsg:4326')\n",
    "\n",
    "    bounds = polygon.total_bounds\n",
    "\n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "url = find_granule_by_point(itslive_catalog, [84.56, 28.54])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc = read_in_s3(url[0])\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading this in as a dask array. Let's take a look at the chunk sizes:\n",
    "\n",
    "**NOTE**: chunksizes shows the largest chunk size. chunks shows the sizes of all chunks along all dims, better if you have irregular chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it could be useful to talk about dask chunk sizes here? Especially since I run into a warning a few steps down. Need to look into rechunking more to better undrestand first -- fix warning below but still not sure about specifying chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CRS of xr object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc.mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the time dimension (`mid_date` here). To start with we'll just print the first 10 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(10):\n",
    "    \n",
    "    print(dc.mid_date[element].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, it doesn't look like the time dimension is in chronological order, let's fix that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted = dc.sortby(dc['mid_date'])\n",
    "dc_timesorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read in the zarr datacube as a `xr.Dataset` we set the chunk sizes to `auto`. When we try to sort along the `mid_date` dimension this seems to become a problem and we get the warning above. \n",
    "\n",
    "Let's follow the instructions in the warning message to avoid creating large chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    dc_timesorted = dc.sortby(dc['mid_date'])\n",
    "    dc_timesorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(10):\n",
    "    \n",
    "    print(dc_timesorted.mid_date[element].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in vector data \n",
    "\n",
    "We are going to read in RGI region **15 (SouthAsiaEast)**. RGI data is downloaded in lat/lon coordinates. We will project it to match the CRS of the ITS_LIVE dataset and then select an individual glacier to begin our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "se_asia = gpd.read_file('/Users/emarshall/Desktop/siparcs/data/nsidc0770_15.rgi60.SouthAsiaEast/15_rgi60_SouthAsiaEast.shp')\n",
    "se_asia.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#project rgi data to match itslive\n",
    "se_asia_prj = se_asia.to_crs('EPSG:32645') #we know the epsg from looking at the 'spatial epsg' attr of the mapping var of the dc object\n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop RGI to ITS_LIVE extent\n",
    "- use get_bbox_single() from access nb but no plotting (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#first, get vector bbox of itslive\n",
    "\n",
    "bbox_dc = get_bbox_single(dc)\n",
    "bbox_dc['geometry']\n",
    "\n",
    "#subset rgi to bounds \n",
    "se_asia_subset = gpd.clip(se_asia_prj, bbox_dc)\n",
    "se_asia_subset\n",
    "se_asia_subset.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_glacier_vec = se_asia_subset.loc[se_asia_subset['RGIId'] == 'RGI60-15.04714']\n",
    "sample_glacier_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip ITS_LIVE dataset to individual glacier extent\n",
    "\n",
    "First, we need to use rio.write_crs() to assign a CRS to the itslive object. If we don't do that first the `rio.clip()` command will produce an error\n",
    "*Note*: it looks like you can only run write_crs() once, because it switches mapping from being a `data_var` to a `coord` so if you run it again it will produce a key error looking for a var that doesnt' exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc_timesorted = dc_timesorted.rio.write_crs(f\"epsg:{dc_timesorted.mapping.attrs['spatial_epsg']}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_glacier_raster = dc_timesorted.rio.clip(sample_glacier_vec.geometry, sample_glacier_vec.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_glacier_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the clipped raster alongside the vector outline. To start with and for the sake of easy visualizing we will take the mean of the magnitude of velocity variable along the `mid_date` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,9))\n",
    "sample_glacier_vec.plot(ax=ax, facecolor='none', edgecolor='red');\n",
    "sample_glacier_raster.v.mean(dim=['mid_date']).plot(ax=ax);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the x and y components of velocity, again averaging over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols =2, figsize=(17,7))\n",
    "\n",
    "sample_glacier_raster.vx.mean(dim='mid_date').plot(ax=axs[0]);\n",
    "sample_glacier_raster.vy.mean(dim='mid_date').plot(ax=axs[1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sample_glacier_raster.v_error.mean(dim=['mid_date']).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring ITS_LIVE data\n",
    "\n",
    "ITS_LIVE data cubes come with many (53!) variables that carry information about the estimated surface velocities and the satellite images that were used to generate the surface velocity estimates. We won't examine all of this information here but let's look at a litte bit.\n",
    "\n",
    "To start with, let's look at the satellite imagery used to generate the velocity data.\n",
    "\n",
    "We see that we have two `data_vars` that indicate which sensor that each image in the image pair at a certain time step comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.satellite_img1.data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.satellite_img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `satellite_img1` and `satellite_img2` variables are 1-dimensional numpy arrays corresponding to the length of the `mid_date` dimension of the data cube. You can see that each element of the array is a string corresponding to a different satellite:\n",
    "    `1A` = Sentinel 1A, `1B` = Sentinel 1B, `2A` = Sentinel 2A\n",
    "    `2B` = Sentinel 2B, `8.` = Landsat8 and `9.` = Landsat9\n",
    "    \n",
    "Let's re-arrange these string arrays into a format that is easier to work with.\n",
    "\n",
    "First, we'll make a set of all the different string values in the satellite image variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_ls1 = list(set(sample_glacier_raster.satellite_img1.compute().data)) #these should be the same, and img1 img2 should only \n",
    "sat_ls2 = set(sample_glacier_raster.satellite_img2.compute().data) #differ if its someting like 2a, 2b or 1a, 1b so i think shouldn't \n",
    "                                                                    #have to worry about the 2 vars ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll assign a value to each element in the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "\n",
    "for x in range(len(sat_ls1)):\n",
    "    mapping[sat_ls1[x]] = x\n",
    "print('mapping: ', mapping)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then convert each element of the satellite image variable arrays to a binary array that gives us the integer associated with each sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert each satellite_img1 value to binary array indicated int associated with sensor\n",
    "one_hot_encode = []\n",
    "for c in sample_glacier_raster.satellite_img1.compute().data:\n",
    "    arr = list(np.zeros(len(sat_ls1), dtype=int))\n",
    "    arr[mapping[c]]= 1\n",
    "    one_hot_encode.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back out the sensor integer from the binary array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ints = [int(one_hot_encode[x].index(1)) for x in range(len(one_hot_encode))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a **pandas dataframe** with each mid_date of the data cube and the sensor integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_ls = list(sample_glacier_raster.mid_date.data)\n",
    "#make dataframe of sensor ints and associated img date\n",
    "sat_df = pd.DataFrame({'mid_date1':dates_ls, 'sensor': sensor_ints})\n",
    "sat_df['mid_date'] = sat_df['mid_date1'].dt.date\n",
    "sat_df = sat_df.drop('mid_date1', axis=1)\n",
    "sat_df = sat_df.sort_values(by='mid_date')\n",
    "sat_df = sat_df.set_index('mid_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's visualize the time series of different sensors as a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap\n",
    "pal = sns.color_palette('Paired',6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,4))\n",
    "sns.heatmap(sat_df.T, cmap=pal, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap those steps into a function to use them more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help from: https://www.educative.io/answers/one-hot-encoding-in-python\n",
    "#help from: https://datascienceparichay.com/article/remove-time-from-date-pandas/\n",
    "\n",
    "def get_satellite_as_int(input_da):\n",
    "    \n",
    "    ''' Function that takes a dast xr.DataArray that represents what sensor velocity data from a specific date was collected from.\n",
    "    returns an xr.DataArray of the sensor coded as an integer key as well as a pandas df with mid_date as index, sensor integer as \n",
    "    a column. **still need to figure out how to carry the mapping of what sensor str corresponds to what sensor integer through**'''\n",
    "    \n",
    "    #make list of satellite strs\n",
    "    sat_ls = list(set(input_da.compute().data))\n",
    "    #sat_ls = list(set(input_da.data))\n",
    "    #map strs to ints\n",
    "    mapping = {}\n",
    "    for x in range(len(sat_ls)):\n",
    "        mapping[sat_ls[x]] = x\n",
    "    print('mapping: ', mapping)\n",
    "    #convert each satellite_img1 value to binary array indicated int associated with sensor\n",
    "    one_hot_encode = []\n",
    "    for c in input_da.compute().data:\n",
    "        arr = list(np.zeros(len(sat_ls), dtype=int))\n",
    "        arr[mapping[c]]= 1\n",
    "        one_hot_encode.append(arr)\n",
    "    sensor_ints = [one_hot_encode[x].index(1) for x in range(len(one_hot_encode))]\n",
    "\n",
    "    dates_ls = list(input_da.mid_date.data)\n",
    "    #make dataframe of sensor ints and associated img date\n",
    "    sat_df = pd.DataFrame({'mid_date1':dates_ls, 'sensor': sensor_ints})\n",
    "    #sat_df['mid_date'] = sat_df['mid_date1'].dt.date\n",
    "    #sat_df = sat_df.drop('mid_date1', axis=1)\n",
    "    sat_df = sat_df.sort_values(by='mid_date1')\n",
    "    sat_df = sat_df.set_index('mid_date1')\n",
    "    \n",
    "    sat_xr = sat_df['sensor'].to_xarray()\n",
    "    \n",
    "    return sat_xr, sat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_satellite_as_int(sample_glacier_raster.satellite_img1.compute())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want we can add this new `xarray.DataArray` back as a `data_var` in the original `xarray.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster['satellite_img_int'] = ('mid_date', a.data)\n",
    "sample_glacier_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining velocity data from each satellite in `ITS_LIVE` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only wanted to look at the velocity estimates from landat8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_data = sample_glacier_raster.where(sample_glacier_raster['satellite_img_int'] == 5., drop=True)\n",
    "l8_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset.where()` at first seems appropriate to use for kind of operation but there's actually an easier way. Because we are selecting along a single dimension (`mid_date`), we can use xarray's `.sel()` method instead. This is more efficient and integrates with `dask` arrays more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_condition = sample_glacier_raster.satellite_img_int.isin(5.)\n",
    "l8_subset = sample_glacier_raster.sel(mid_date=l8_condition)\n",
    "l8_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are looking at roughly a third of the original time steps. Let's take a look at the average speeds of the Landsat8-derived velocities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Landsat9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_condition = sample_glacier_raster.satellite_img_int.isin(1.)\n",
    "\n",
    "l9_subset = sample_glacier_raster.sel(mid_date=l9_condition)\n",
    "l9_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 45 time steps have data from Landsat9, this makes sense because Landsat9 was just launched recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Sentinel 1 data. Note here we are selecting for 2 values instead of 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_condition = sample_glacier_raster.satellite_img_int.isin([0,2])\n",
    "s1_subset = sample_glacier_raster.sel(mid_date = s1_condition)\n",
    "s1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_condition = sample_glacier_raster.satellite_img_int.isin([3,4])\n",
    "s2_subset = sample_glacier_raster.sel(mid_date=s2_condition)\n",
    "s2_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt at ufunc, didn't work\n",
    "\n",
    "#def xr_satellite_coding(a):\n",
    "#    return xr.apply_ufunc(get_satellite_as_int, a)\n",
    "#sample_glacier_raster\n",
    "#test_ds = xr_satellite_coding(sample_glacier_raster.satellite_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette('Paired',6)\n",
    "pal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal mean velocities with groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#first define the function we'll apply to each group\n",
    "def middate_mean(a):\n",
    "    return a.mean(dim='mid_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "seasons_gb = sample_glacier_raster.groupby(sample_glacier_raster.mid_date.dt.season).map(middate_mean)\n",
    "#add attrs to gb object\n",
    "seasons_gb.attrs = sample_glacier_raster.attrs #why didn't that work?\n",
    "seasons_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fg = seasons_gb.v.plot(\n",
    "    col='season',\n",
    "    vmax = 150\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewbook",
   "language": "python",
   "name": "mynewbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
