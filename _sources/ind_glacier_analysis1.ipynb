{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual glacier analysis 1\n",
    "\n",
    "This notebook will walk you through steps to read in and organize velocity data in a raster format using xarray and rioxarray tools\n",
    "\n",
    "First, lets install the python libraries that were listed on the [Software](software.ipynb) page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import json\n",
    "import urllib.request\n",
    "from skimage.morphology import skeletonize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing ITS_LIVE data stored in s3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url_catalog:\n",
    "    itslive_catalog = json.loads(url_catalog.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a single catalog entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive_catalog['features'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to find the url that corresponds to the zarr datacube for a specific point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_granule_by_point(input_dict, input_point): #[lon,lat]\n",
    "    '''Takes an inputu dictionary (a geojson catalog) and a point to represent AOI.\n",
    "    this returns a list of the s3 urls corresponding to zarr datacubes whose footprint covers the AOI'''\n",
    "    #print([input_points][0])\n",
    "    \n",
    "    target_granule_urls = []\n",
    "    #Point(coord[0], coord[1])\n",
    "    #print(input_point[0])\n",
    "    #print(input_point[1])\n",
    "    point_geom = Point(input_point[0], input_point[1])\n",
    "    #print(point_geom)\n",
    "    point_gdf = gpd.GeoDataFrame(crs='epsg:4326', geometry = [point_geom])\n",
    "    for granule in range(len(input_dict['features'])):\n",
    "        \n",
    "        #print('tick')\n",
    "        bbox_ls = input_dict['features'][granule]['geometry']['coordinates'][0]\n",
    "        bbox_geom = Polygon(bbox_ls)\n",
    "        bbox_gdf = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry = [bbox_geom])\n",
    "        \n",
    "        #if poly_gdf.contains(points1_ls[poly]).all() == True:\n",
    "\n",
    "        if bbox_gdf.contains(point_gdf).all() == True:\n",
    "            #print('yes')\n",
    "            target_granule_urls.append(input_dict['features'][granule]['properties']['zarr_url'])\n",
    "        else:\n",
    "            pass\n",
    "            #print('no')\n",
    "    return target_granule_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will read in a xarray dataset from a url to a zarr datacube when we're ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_s3(http_url):\n",
    "    s3_url = http_url.replace('http','s3')\n",
    "    s3_url = s3_url.replace('.s3.amazonaws.com','')\n",
    "\n",
    "    datacube = xr.open_dataset(s3_url, engine = 'zarr',\n",
    "                                storage_options={'anon':True},\n",
    "                                chunks = 'auto')\n",
    "\n",
    "    return datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_single(input_xr):\n",
    "    \n",
    "    '''Takes input xr object (from itslive data cube), plots a quick map of the footprint. \n",
    "    currently only working for granules in crs epsg 32645'''\n",
    "\n",
    "    xmin = input_xr.coords['x'].data.min()\n",
    "    xmax = input_xr.coords['x'].data.max()\n",
    "\n",
    "    ymin = input_xr.coords['y'].data.min()\n",
    "    ymax = input_xr.coords['y'].data.max()\n",
    "\n",
    "    pts_ls = [(xmin, ymin), (xmax, ymin),(xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "    #print(input_xr.mapping.spatial_epsg)\n",
    "    #print(f\"epsg:{input_xr.mapping.spatial_epsg}\")\n",
    "    crs = f\"epsg:{input_xr.mapping.spatial_epsg}\"\n",
    "    #crs = {'init':f'epsg:{input_xr.mapping.spatial_epsg}'}\n",
    "    #crs = 'epsg:32645'\n",
    "    #print(crs)\n",
    "\n",
    "    polygon_geom = Polygon(pts_ls)\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom]) \n",
    "    #polygon = polygon.to_crs('epsg:4326')\n",
    "\n",
    "    bounds = polygon.total_bounds\n",
    "\n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = find_granule_by_point(itslive_catalog, [84.56, 28.54])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = read_in_s3(url[0])\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CRS of xr object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in vector data (Randolph Glacier Inventory)\n",
    "- this is downloaded locally from the [NSIDC](https://nsidc.org/data/nsidc-0770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_file('/Users/emarshall/Desktop/siparcs/data/nsidc0770_15.rgi60.SouthAsiaEast/15_rgi60_SouthAsiaEast.shp')\n",
    "se_asia.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project rgi data to match itslive\n",
    "se_asia_prj = se_asia.to_crs('EPSG:32645') #we know the epsg from looking at the 'spatial epsg' attr of the mapping var of the dc object\n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop RGI to ITS_LIVE extent\n",
    "- use get_bbox_single() from access nb but no plotting (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, get vector bbox of itslive\n",
    "\n",
    "bbox_dc = get_bbox_single(dc)\n",
    "bbox_dc['geometry']\n",
    "\n",
    "#subset rgi to bounds \n",
    "se_asia_subset = gpd.clip(se_asia_prj, bbox_dc)\n",
    "se_asia_subset\n",
    "se_asia_subset.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find largest glacier (just to start)\n",
    "se_asia_subset.sort_values('Area', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_glacier_vec = se_asia_subset.loc[se_asia_subset['Area'] == 42.398]\n",
    "largest_glacier_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip ITSLIVE dataset to individual glacier extent\n",
    "\n",
    "First, we need to use rio.write_crs() to assign a CRS to the itslive object. If we don't do that first the `rio.clip()` command will produce an error\n",
    "*Note*: it looks like you can only run write_crs() once, because it switches mapping from beign a `data_var` to a `coord` so if you run it again it will produce a key error looking for a var that doesnt' exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.rio.write_crs(f\"epsg:{dc.mapping.attrs['spatial_epsg']}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "largest_glacier_raster = dc.rio.clip(largest_glacier_vec.geometry, largest_glacier_vec.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_glacier_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_glacier_raster.isel(mid_date=0).v.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_glacier_raster.v_error.isel(mid_date=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_glacier_raster.v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal mean velocities with groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first define the function we'll apply to each group\n",
    "def middate_mean(a):\n",
    "    return a.mean(dim='mid_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_gb = largest_glacier_raster.groupby(largest_glacier_raster.mid_date.dt.season).map(middate_mean)\n",
    "#add attrs to gb object\n",
    "seasons_gb.attrs = largest_glacier_raster.attrs #why didn't that work?\n",
    "seasons_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = seasons_gb.v.plot(\n",
    "    col='season',\n",
    "    vmax = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier centerline extraction\n",
    "\n",
    "-using scikit-image skeletonize()\n",
    "- probably not the best/ideal way to do this, for now wanted to have an example but maybe not worth including here? \n",
    "\n",
    "Let's choose a different glacier to focus on this time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_04118 = se_asia_subset.loc[se_asia_subset['RGIId'] == 'RGI60-15.04118']\n",
    "glacier_04118.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the same steps of clipping the full itslive dataset to the extent of the glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_04118 = dc.rio.clip(glacier_04118.geometry, glacier_04118.crs)\n",
    "raster_04118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the skimage function `skeletonize()` to extract a centerline (this is a rough approximation for the purposes of this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_04118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewbook",
   "language": "python",
   "name": "mynewbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
